[{"authors":null,"categories":null,"content":"‚ÄúBe Boundless.‚Äù\nHaotian Zhang is an incoming Research Scientist at Apple AI/ML, Visual Intelligence. His research aims to enable embodied agents to understand the outside world. To that end, he works on designing sensible modules that learn the effective representation of information from 2D/3D image data, as well as natural language. His recent work on GLIP\u0026amp;GLIPv2 has been accepted to the CVPR 2022 (Best Paper Finalist), and NeurIPS 2022. He also co-organized the ECCV 2022 workshop on Computer Vision in the Wild.\nPrior to joining Apple, he obtained his Ph.D. in the Information Processing Lab at University of Washington, advised by Prof. Jenq-Neng Hwang, where he focused on monocular 3D object detection and multi-object tracking. He received his B.S. degree at Shanghai Jiao Tong University in 2017, supervised by Prof. Jun-Fa Mao.\nHe believes that living an interesting life is done by doing interesting things with interesting people, and that‚Äôs what he hopes to do üî•.\nDownload CV here.\n","date":1654992e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1654992e3,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"‚ÄúBe Boundless.‚Äù\nHaotian Zhang is an incoming Research Scientist at Apple AI/ML, Visual Intelligence. His research aims to enable embodied agents to understand the outside world. To that end, he works on designing sensible modules that learn the effective representation of information from 2D/3D image data, as well as natural language.","tags":null,"title":"Haotian Zhang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://haotian-zhang.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Haotian Zhang","Pengchuan Zhang","Xiaowei Hu","Yen-Chun Chen","Liunian Harold Li","Xiyang Dai","Lijuan Wang","Lu Yuan","Jenq-Neng Hwang","Jianfeng Gao"],"categories":null,"content":" ","date":1654992e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654992e3,"objectID":"4a5f5b746dd4b8afb5292e319d982de7","permalink":"https://haotian-zhang.github.io/publication/glipv2/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/glipv2/","section":"publication","summary":"We present GLIPv2, a grounded VL understanding model, that serves both localization tasks (e.g., object detection, instance segmentation) and Vision-Language (VL) understanding tasks (e.g., VQA, image captioning).","tags":[],"title":"GLIPv2: Unifying Localization and Vision-Language Understanding","type":"publication"},{"authors":["Liunian Harold Li","Pengchuan Zhang","Haotian Zhang","Jianwei Yang","Chunyuan Li","Yiwu Zhong","Lijuan Wang","Lu Yuan","Lei Zhang","Jenq-Neng Hwang","Kai-Wei Chang","Jianfeng Gao"],"categories":null,"content":" ","date":1647043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647043200,"objectID":"dbab8f55a762534877c40113ea693a8c","permalink":"https://haotian-zhang.github.io/publication/glip/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/glip/","section":"publication","summary":"This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training.","tags":[],"title":"GLIP: Grounded Language-Image Pre-training","type":"publication"},{"authors":["Gaoang Wang","Yizhou Wang","Haotian Zhang","Renshu Gu","Jenq-Neng Hwang"],"categories":null,"content":" ","date":1571097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571097600,"objectID":"2a09b01147e82273545635a63235c876","permalink":"https://haotian-zhang.github.io/publication/tnt/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/tnt/","section":"publication","summary":"In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework.","tags":[],"title":"Exploit the connectivity: Multi-object tracking with trackletnet","type":"publication"},{"authors":["Haotian Zhang","Gaoang Wang","Zhichao Lei","Jenq-Neng Hwang"],"categories":null,"content":" ","date":1571097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571097600,"objectID":"5bcb521720fd3140b460f04720ce97ed","permalink":"https://haotian-zhang.github.io/publication/eye_drone/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/eye_drone/","section":"publication","summary":"In this paper, a drone-based multi-object tracking and 3D localization scheme is proposed based on the deep learning based object detection.","tags":[],"title":"Eye in the sky: Drone-based object tracking and 3D localization","type":"publication"},{"authors":null,"categories":null,"content":"\r[10/2022] Serving as session co-chair for ECCV CVinW Workshop and being responsible for ODinW. Full schedule here: https://computer-vision-in-the-wild.github.io/eccv-2022/.\n[10/2022] Selected as one of the Young Scholar Award recipients for NeurIPS 2022.\n[09/2022] One paper accepted by NeurIPS 2022: GLIPv2. A team effort to push CVinW\n[09/2022] Organizing ECCV Workshop Computer Vision in the Wild (CVinW), where two challenges Image Classification in the Wild (ICinW) and Object Detection in the Wild (ODinW) are hosted to evaluate the zero-shot, few-shot and full-shot performance of pre-trained vision models.\n[03/2022] One paper accepted by CVPR 2022: GLIP as an Oral \u0026amp; Best Paper Finalist.\n[10/2021] I am the üèÜwinner of Video Track (in both MOTChallenge-STEP and KITTI-STEP dataset) in the 6th BMTT Challenge (in conjunction with ICCV 2021)!\n[06/2020] Our team is the winner of track 3 (multi-object tracking and segmentation in KITTI-MOTS and MOTS20 dataset with public detection) and the runner-up of track 2 (multi-object detection, tracking and segmentation in KITTI-MOTS dataset) in the 5th BMTT Challenge in CVPR 2020 workshop. [Details‚Ä¶]\n","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"https://haotian-zhang.github.io/news/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"List of news.\r\n","tags":[],"title":"News","type":"page"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://haotian-zhang.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://haotian-zhang.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]