<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: October 28, 2022 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.e5d7adca760216d3b7e28ea434e81f6f.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Haotian Zhang"><meta name=description content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><link rel=alternate hreflang=en-us href=https://haotian-zhang.github.io/><link rel=canonical href=https://haotian-zhang.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0a0e24d5fd4170ac8f3265a78438ac6a_75220_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0a0e24d5fd4170ac8f3265a78438ac6a_75220_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://haotian-zhang.github.io/media/icon_hu0a0e24d5fd4170ac8f3265a78438ac6a_75220_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Haotian Zhang"><meta property="og:url" content="https://haotian-zhang.github.io/"><meta property="og:title" content="Haotian Zhang"><meta property="og:description" content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><meta property="og:image" content="https://haotian-zhang.github.io/media/icon_hu0a0e24d5fd4170ac8f3265a78438ac6a_75220_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2030-06-01T13:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://haotian-zhang.github.io?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://haotian-zhang.github.io"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Haotian Zhang"><title>Haotian Zhang</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Haotian Zhang</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Haotian Zhang</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured data-target=#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experience data-target=#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/uploads/resume.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/HaotianZhang4AI data-toggle=tooltip data-placement=bottom title=HaotianZhang4AI target=_blank rel=noopener aria-label=HaotianZhang4AI><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/authors/admin/avatar_huf328b66350c00d1753d1a927fc5c8c35_75703_270x270_fill_q75_lanczos_center.jpg alt="Haotian Zhang"><div class=portrait-title><h2>Haotian Zhang</h2><h3>Research Scientist at Apple<br>Former Research Intern at MSR AI<br>Ph.D. at University of Washington</h3><h3><a href=https://machinelearning.apple.com/ target=_blank rel=noopener><span>Apple AI/ML</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=mailto:haotian.carl.zhang@gmail.com aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://twitter.com/HaotianZhang4AI target=_blank rel=noopener aria-label=twitter><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?user=1vz0kKUAAAAJ&hl=en" target=_blank rel=noopener aria-label=graduation-cap><i class="fas fa-graduation-cap big-icon"></i></a></li><li><a href=https://github.com/Haotian-Zhang target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=https://www.linkedin.com/in/haotian-zhang-075508a6/ target=_blank rel=noopener aria-label=linkedin><i class="fab fa-linkedin big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Biography</h1><div class=article-style><p>&ldquo;Be Boundless.&rdquo;</p><p>Haotian Zhang is an incoming Research Scientist at <a href="https://machinelearning.apple.com/research?page=1&domain=Computer+Vision" target=_blank rel=noopener>Apple AI/ML, Visual Intelligence</a>. His research aims to enable embodied agents to understand the outside world. To that end, he works on designing sensible modules that learn the effective representation of information from 2D/3D image data, as well as natural language. His recent work on <a href=https://www.microsoft.com/en-us/research/project/project-florence-vl/articles/object-detection-in-the-wild-via-grounded-language-image-pre-training/ target=_blank rel=noopener>GLIP&GLIPv2</a> has been accepted to the CVPR 2022 (Best Paper Finalist), and NeurIPS 2022. He also co-organized the ECCV 2022 workshop on Computer Vision in the Wild.</p><p>Prior to joining Apple, he obtained his Ph.D. in the <a href=https://ipl-uw.github.io/ target=_blank rel=noopener>Information Processing Lab</a> at <a href=https://www.washington.edu/ target=_blank rel=noopener>University of Washington</a>, advised by Prof. <a href=https://people.ece.uw.edu/hwang/ target=_blank rel=noopener>Jenq-Neng Hwang</a>, where he focused on monocular 3D object detection and multi-object tracking. He received his B.S. degree at Shanghai Jiao Tong University in 2017, supervised by Prof. <a href=https://en.wikipedia.org/wiki/Mao_Junfa target=_blank rel=noopener>Jun-Fa Mao</a>.</p><p><span style=color:orange>He believes that living an interesting life is done by doing interesting things with interesting people, and that’s what he hopes to do 🔥.</span></p><p><i class="fas fa-download pr-1 fa-fw"></i> Download
<a href=/uploads/resume.pdf target=_blank>CV</a>
here.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests mb-0"><li>Open-Vocabulary Object Detection</li><li>Vision-and-Language Pre-training</li><li>Large-scaled Pre-trained Models</li><li>3D Monocular Object Detection</li><li>2D/3D Multi-object Tracking</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>PhD in Elecricial & Computer Engineering, 2022</p><p class=institution>University of Washington</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>MSc in Applied Mathematics, 2021</p><p class=institution>University of Washington</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>MSc in Elecricial & Computer Engineering, 2019</p><p class=institution>University of Washington</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BSc in Nano & Microelectronics, 2017</p><p class=institution>Shanghai Jiao Tong University</p></div></li></ul></div></div></div></div></div></section><section id=recent_news class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent News</h1><p class=mt-1><a href=/news>All news&#187;</a></p></div><div class="col-12 col-lg-8"><p><strong>[10/2022]</strong> Serving as session co-chair for ECCV CVinW Workshop and being responsible for ODinW. Full schedule here: <a href=https://computer-vision-in-the-wild.github.io/eccv-2022/ target=_blank rel=noopener>https://computer-vision-in-the-wild.github.io/eccv-2022/</a>.</p><p><strong>[10/2022]</strong> Selected as one of the Young Scholar Award recipients for NeurIPS 2022.</p><p><strong>[09/2022]</strong> One paper accepted by NeurIPS 2022: <a href=https://arxiv.org/abs/2206.05836 target=_blank rel=noopener>GLIPv2</a>. A team effort to push <a href=https://computer-vision-in-the-wild.github.io/eccv-2022/ target=_blank rel=noopener>CVinW</a></p><p><strong>[08/2022]</strong> Updated GLIP <a href=https://huggingface.co/spaces/haotiz/glip-zeroshot-demo target=_blank rel=noopener>Hugging Face Gradio Demo</a>! Feel free to check it out!!!</p><p><strong>[09/2022]</strong> Organizing ECCV Workshop <a href=https://computer-vision-in-the-wild.github.io/eccv-2022/ target=_blank rel=noopener><em>Computer Vision in the Wild (CVinW)</em></a>, where two challenges <a href=https://eval.ai/web/challenges/challenge-page/1832/overview target=_blank rel=noopener><em>Image Classification in the Wild (ICinW)</em></a> and <a href=https://eval.ai/web/challenges/challenge-page/1839/overview target=_blank rel=noopener><em>Object Detection in the Wild (ODinW)</em></a> are hosted to evaluate the zero-shot, few-shot and full-shot performance of pre-trained vision models.</p><p><strong>[03/2022]</strong> One paper accepted by CVPR 2022: <a href=https://arxiv.org/abs/2112.03857 target=_blank rel=noopener>GLIP</a> as an Oral & Best Paper Finalist.</p></div></div></div></section><section id=featured class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Featured Publications</h1></div><div class="col-12 col-lg-8"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Haotian Zhang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span>Pengchuan Zhang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span>Xiaowei Hu</span>, <span>Yen-Chun Chen</span>, <span>Liunian Harold Li</span>, <span>Xiyang Dai</span>, <span>Lijuan Wang</span>, <span>Lu Yuan</span>, <span>Jenq-Neng Hwang</span>, <span>Jianfeng Gao</span></span>
(2022).
<a href=/publication/glipv2/>GLIPv2: Unifying Localization and Vision-Language Understanding</a>.
In <em>NeurIPS 2022</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2206.05836.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/glipv2/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/microsoft/GLIP target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.microsoft.com/en-us/research/project/project-florence-vl/articles/object-detection-in-the-wild-via-grounded-language-image-pre-training/ target=_blank rel=noopener>Project</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Liunian Harold Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span>Pengchuan Zhang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span class=author-highlighted>Haotian Zhang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="equal contribution"></i>, <span>Jianwei Yang</span>, <span>Chunyuan Li</span>, <span>Yiwu Zhong</span>, <span>Lijuan Wang</span>, <span>Lu Yuan</span>, <span>Lei Zhang</span>, <span>Jenq-Neng Hwang</span>, <span>Kai-Wei Chang</span>, <span>Jianfeng Gao</span></span>
(2022).
<a href=/publication/glip/>GLIP: Grounded Language-Image Pre-training</a>.
In <em>CVPR 2022</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2112.03857 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/glip/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/microsoft/GLIP target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.microsoft.com/en-us/research/project/project-florence-vl/articles/object-detection-in-the-wild-via-grounded-language-image-pre-training/ target=_blank rel=noopener>Project</a></p></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=experience class="home-section wg-experience"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Working Experience</h1></div><div class="col-12 col-lg-8"><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border exp-fill">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://machinelearning.apple.com/ target=_blank rel=noopener><img src=/media/icons/brands/org-apple.svg width=56px height=56px alt="Apple AI/ML" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Scientist, Visual Intelligence</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://machinelearning.apple.com/ target=_blank rel=noopener>Apple AI/ML</a></div><div class="text-muted exp-meta">Jan 2023 –
Present
<span class=middot-divider></span>
<span>Cuppertino, California</span></div></div></div><div class=card-text>Research scientist @ Visual Intelligence Team, directed by <a href=https://sites.google.com/site/yinfeiyang/ target=_blank rel=noopener>Yinfei Yang</a>. I will continue pushing the boundary of CV (OD) and Multi-modal intelligence on my new position with this great team.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.microsoft.com/en-us/research/group/deep-learning-group/ target=_blank rel=noopener><img src=/media/icons/brands/org-ms.svg width=56px height=56px alt="Microsoft Research" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern, Deep Learning</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.microsoft.com/en-us/research/group/deep-learning-group/ target=_blank rel=noopener>Microsoft Research</a></div><div class="text-muted exp-meta">Jun 2021 –
Mar 2022
<span class=middot-divider></span>
<span>Redmond, Washington</span></div></div></div><div class=card-text>Research Intern @ <a href=https://www.microsoft.com/en-us/research/group/deep-learning-group/ target=_blank rel=noopener>Deep Learning Group</a>, mentored by <a href=https://pzzhang.github.io/pzzhang/ target=_blank rel=noopener>Pengchuan Zhang</a>, <a href=https://jwyang.github.io/ target=_blank rel=noopener>Jianwei Yang</a>, <a href=https://chunyuan.li/ target=_blank rel=noopener>Chunyuan Li</a>, and <a href=https://www.microsoft.com/en-us/research/people/jfgao/ target=_blank rel=noopener>Jianfeng Gao</a>. It&rsquo;s my great honor and pleasure to work with such a talented team.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://azure.microsoft.com/en-us/solutions/ai/ target=_blank rel=noopener><img src=/media/icons/brands/org-ms.svg width=56px height=56px alt="Azure AI" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern, Computer Vision</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://azure.microsoft.com/en-us/solutions/ai/ target=_blank rel=noopener>Azure AI</a></div><div class="text-muted exp-meta">Jun 2021 –
Sep 2021
<span class=middot-divider></span>
<span>Redmond, Washington</span></div></div></div><div class=card-text>Research Intern @ <a href=https://www.microsoft.com/en-us/research/project/document-ai/ target=_blank rel=noopener>Visual Document Intelligence Team</a>, mentored by <a href=https://www.microsoft.com/en-us/research/people/dinei/ target=_blank rel=noopener>Dinei Florencio</a>, <a href=https://www.linkedin.com/in/yijuan-lu-590b426/ target=_blank rel=noopener>Yijuan Lu</a>, and <a href=https://www.guoxwang.com/ target=_blank rel=noopener>Guoxin Wang</a>. I appreciate their helpful guidance and suggestions during the internship.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border exp-fill">&nbsp;</span></div><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.ece.uw.edu/ target=_blank rel=noopener><img src=/media/icons/brands/org-uw.svg width=56px height=56px alt="University of Washington" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Ph.D. student, ECE</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.ece.uw.edu/ target=_blank rel=noopener>University of Washington</a></div><div class="text-muted exp-meta">Sep 2017 –
Present
<span class=middot-divider></span>
<span>Seattle, Washington</span></div></div></div><div class=card-text>Ph.D. student @ <a href=https://ipl-uw.github.io/ target=_blank rel=noopener>Information Processing Lab</a>, supervised by Prof. <a href=https://people.ece.uw.edu/hwang/ target=_blank rel=noopener>Jenq-Neng Hwang</a>. <span style=color:gold>&ldquo;A teacher for a day is a father for a lifetime.&rdquo; 👨‍🏫</span></div></div></div></div></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>